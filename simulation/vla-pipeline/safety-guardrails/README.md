# Safety Guardrails for LLM-Controlled Robots

This module implements safety guardrails for the Vision-Language-Action (VLA) system in the Physical AI & Humanoid Robotics textbook. The safety system ensures that commands generated by LLMs are executed safely by the robot.

## Overview

The safety guardrails system operates as an intermediary between the high-level VLA commands and the robot's control system. It monitors sensor data and robot state to ensure that all commands are safe to execute before forwarding them to the robot.

### Key Features

1. **Obstacle Detection and Avoidance**: Continuously monitors LIDAR and camera data to detect obstacles in the robot's path
2. **Speed Limiting**: Ensures the robot does not exceed safe velocity limits
3. **Emergency Stop**: Activates emergency stop if a collision is imminent
4. **Command Filtering**: Validates and filters potentially unsafe commands from the LLM
5. **Safety Intervention Logging**: Tracks safety interventions for analysis

## Architecture

```
LLM/VLA Command → Safety Guardrails → Robot Control
      ↑                      ↓
   Unsafe Cmd            Safe Cmd
      ↓                      ↑
   Sensor Data ←——————— Sensor Data
```

The system sits between the VLA command generator and the robot's control system, intercepting all commands and ensuring they are safe to execute based on real-time sensor data.

## Components

### 1. Safety Node (`safety_guardrails.py`)
The main safety node that processes incoming commands and validates them against sensor data.

**Topics Subscribed:**
- `/unsafe_cmd_vel` - Incoming potentially unsafe commands from VLA system
- `/scan` - LIDAR scan data for obstacle detection
- `/odom` - Robot odometry for position tracking

**Topics Published:**
- `/safe_cmd_vel` - Validated safe commands for robot execution
- `/emergency_stop` - Emergency stop signals
- `/safety/status` - Safety system status information

### 2. Launch File (`launch/safety_guardrails.launch.py`)
Launch file to start the safety guardrails system with configurable parameters.

## Configuration Parameters

- `min_obstacle_distance`: Minimum safe distance to obstacles (default: 0.5m)
- `max_linear_speed`: Maximum allowed linear velocity (default: 0.5 m/s)
- `max_angular_speed`: Maximum allowed angular velocity (default: 1.0 rad/s)
- `collision_threshold`: Distance at which collision is imminent (default: 0.3m)
- `emergency_stop_distance`: Distance for emergency stop activation (default: 0.2m)

## Usage

### Running the Safety System

```bash
# Launch the safety guardrails system
ros2 launch physical_ai_robotics safety_guardrails.launch.py
```

### Integration with VLA Pipeline

The safety system should be integrated into the VLA pipeline as follows:

1. VLA system generates commands and publishes to `/unsafe_cmd_vel`
2. Safety guardrails node validates the commands against sensor data
3. Validated commands are published to `/safe_cmd_vel` for robot execution
4. Robot subscribes to `/safe_cmd_vel` instead of direct commands

## Safety Validation

The system continuously performs the following safety checks:

1. **Obstacle Proximity Check**: Ensures no obstacles are within the minimum safe distance in the direction of movement
2. **Speed Limit Check**: Verifies commanded velocities are within safe limits
3. **Collision Prediction**: Predicts if the current command trajectory will result in a collision
4. **Emergency Override**: Provides emergency stop capability if needed

## Performance Metrics

The safety system tracks and reports the following metrics:

- Total commands processed
- Number of safety interventions
- Intervention rate (percentage of commands that required safety correction)
- Average response time for safety validation

## Example Integration

```python
# In your VLA system, publish to the safety system instead of directly to robot
import rclpy
from geometry_msgs.msg import Twist
from rclpy.node import Node

class VLAExampleNode(Node):
    def __init__(self):
        super().__init__('vla_example_node')

        # Publish to safety system instead of directly to robot
        self.cmd_pub = self.create_publisher(Twist, '/unsafe_cmd_vel', 10)

        # Timer to send example commands
        self.timer = self.create_timer(1.0, self.send_command)

    def send_command(self):
        cmd = Twist()
        cmd.linear.x = 0.3  # Forward at 0.3 m/s
        cmd.angular.z = 0.1  # Slight turn

        # This command will be validated by the safety system
        self.cmd_pub.publish(cmd)
```

## Testing

The safety system includes various test scenarios to validate its functionality:

1. **Obstacle Avoidance Test**: Place obstacles in the robot's path and verify the system prevents collision
2. **Speed Limit Test**: Send commands exceeding speed limits and verify they are limited
3. **Emergency Stop Test**: Create scenarios requiring emergency stop and verify system response
4. **Normal Operation Test**: Verify normal commands pass through unchanged when safe

## Troubleshooting

### Commands not executing
- Check that the safety node is running
- Verify topic remappings are correct
- Ensure sensor data (LIDAR/odometry) is available

### Excessive safety interventions
- Adjust `min_obstacle_distance` parameter if it's too conservative
- Verify sensor data quality
- Check for sensor calibration issues

### Performance issues
- Reduce the frequency of safety checks if needed
- Optimize obstacle detection algorithms for your specific hardware

## Safety Considerations

This safety system provides a software-based safety layer but should not be considered a complete safety solution. Additional safety measures may be required for real robot deployment:

- Physical safety hardware
- Hard limits on actuators
- Mechanical safety systems
- Human oversight protocols

## Integration with Textbook

This safety system is designed to work with the Vision-Language-Action (VLA) pipeline described in Module 4 of the Physical AI & Humanoid Robotics textbook. It demonstrates how to implement safety measures when integrating LLM-generated commands with robotic systems.